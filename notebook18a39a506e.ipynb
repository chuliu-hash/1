{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nfrom PIL import Image\nimport os\n\n# 定义残差块\nclass BasicBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_channels != out_channels:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out += self.shortcut(x)\n        out = self.relu(out)\n        return out\n\n# 定义 ResNet-18 模型\nclass ResNet(nn.Module):\n    def __init__(self, block, num_classes=10):\n        super(ResNet, self).__init__()\n        self.in_channels = 64\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        # 定义各个层\n        self.layer1 = self._make_layer(block, 64, 2, stride=1)\n        self.layer2 = self._make_layer(block, 128, 2, stride=2)\n        self.layer3 = self._make_layer(block, 256, 2, stride=2)\n        self.layer4 = self._make_layer(block, 512, 2, stride=2)\n\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512, num_classes)\n\n    def _make_layer(self, block, out_channels, blocks, stride):\n        layers = []\n        layers.append(block(self.in_channels, out_channels, stride))\n        self.in_channels = out_channels\n        for _ in range(1, blocks):\n            layers.append(block(out_channels, out_channels))\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n        return x","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-30T15:01:48.771286Z","iopub.execute_input":"2024-09-30T15:01:48.771928Z","iopub.status.idle":"2024-09-30T15:01:48.791295Z","shell.execute_reply.started":"2024-09-30T15:01:48.771886Z","shell.execute_reply":"2024-09-30T15:01:48.790382Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"  \n# 超参数\nbatch_size = 64\nnum_epochs = 20\nlearning_rate = 0.001\n\n# 数据预处理和增强\ntransform = transforms.Compose([\n    transforms.RandomHorizontalFlip(),  # 随机水平翻转\n    transforms.RandomRotation(10),      # 随机旋转\n    transforms.RandomCrop(32, padding=4),  # 随机裁剪，加上填充\n    transforms.Resize((224, 224)),      # 将图像调整为224x224\n    transforms.ToTensor(),               # 转为Tensor\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),  # 归一化\n])\n\n# 下载和加载 CIFAR-10 数据集\ntrain_dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n                                             download=True, transform=transform)\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n\ntest_dataset = torchvision.datasets.CIFAR10(root='./data', train=False,\n                                            download=True, transform=transform)\ntest_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n\n# 初始化模型\nmodel = ResNet(BasicBlock)\n\n# 设置设备\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\n# 定义损失函数和优化器\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-30T15:01:48.792833Z","iopub.execute_input":"2024-09-30T15:01:48.793113Z","iopub.status.idle":"2024-09-30T15:01:50.473443Z","shell.execute_reply.started":"2024-09-30T15:01:48.793082Z","shell.execute_reply":"2024-09-30T15:01:50.472682Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\n","output_type":"stream"}]},{"cell_type":"code","source":"# 保存检查点\ndef save_checkpoint(model, optimizer, epoch):\n    checkpoint_filename = f'checkpoint_epoch_{epoch}.pth'\n    torch.save({\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n    }, checkpoint_filename)\n    print(f\"Checkpoint saved: {checkpoint_filename}\")\n\n# 加载模型检查点\ndef load_checkpoint(model, optimizer, checkpoint_path):\n    if os.path.isfile(checkpoint_path):\n        checkpoint = torch.load(checkpoint_path)\n        model.load_state_dict(checkpoint['model_state_dict'])\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        epoch = checkpoint['epoch']\n        print(f\"Loaded checkpoint from epoch {epoch}\")\n    else:\n        print(\"No checkpoint found.\")\n\n\nload_checkpoint(model, optimizer, 'checkpoint_epoch_0.pth')  # 可选的最新检查点\n\n# 训练模型\nfor epoch in range(num_epochs):\n    model.train()\n    for i, (images, labels) in enumerate(train_loader):\n        images, labels = images.to(device), labels.to(device)\n\n        # 前向传播\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n\n        # 后向传播和优化\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if (i + 1) % 100 == 0:\n            print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n    \n    # 保存检查点\n    save_checkpoint(model, optimizer, epoch + 1)  # 保存每个 epoch 的检查点\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-30T15:01:50.474584Z","iopub.execute_input":"2024-09-30T15:01:50.474905Z","iopub.status.idle":"2024-09-30T15:49:38.475819Z","shell.execute_reply.started":"2024-09-30T15:01:50.474871Z","shell.execute_reply":"2024-09-30T15:49:38.474760Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"No checkpoint found.\nEpoch [1/20], Step [100/782], Loss: 1.9884\nEpoch [1/20], Step [200/782], Loss: 1.7790\nEpoch [1/20], Step [300/782], Loss: 1.5558\nEpoch [1/20], Step [400/782], Loss: 1.3486\nEpoch [1/20], Step [500/782], Loss: 1.5597\nEpoch [1/20], Step [600/782], Loss: 1.1876\nEpoch [1/20], Step [700/782], Loss: 1.5459\nCheckpoint saved: checkpoint_epoch_1.pth\nEpoch [2/20], Step [100/782], Loss: 1.0549\nEpoch [2/20], Step [200/782], Loss: 1.1639\nEpoch [2/20], Step [300/782], Loss: 0.8396\nEpoch [2/20], Step [400/782], Loss: 1.3172\nEpoch [2/20], Step [500/782], Loss: 1.2901\nEpoch [2/20], Step [600/782], Loss: 1.1530\nEpoch [2/20], Step [700/782], Loss: 1.0441\nCheckpoint saved: checkpoint_epoch_2.pth\nEpoch [3/20], Step [100/782], Loss: 1.0583\nEpoch [3/20], Step [200/782], Loss: 0.8677\nEpoch [3/20], Step [300/782], Loss: 0.7281\nEpoch [3/20], Step [400/782], Loss: 0.8246\nEpoch [3/20], Step [500/782], Loss: 0.8043\nEpoch [3/20], Step [600/782], Loss: 0.7541\nEpoch [3/20], Step [700/782], Loss: 0.7992\nCheckpoint saved: checkpoint_epoch_3.pth\nEpoch [4/20], Step [100/782], Loss: 0.9765\nEpoch [4/20], Step [200/782], Loss: 0.6753\nEpoch [4/20], Step [300/782], Loss: 0.6620\nEpoch [4/20], Step [400/782], Loss: 0.8632\nEpoch [4/20], Step [500/782], Loss: 0.4980\nEpoch [4/20], Step [600/782], Loss: 0.6231\nEpoch [4/20], Step [700/782], Loss: 0.8491\nCheckpoint saved: checkpoint_epoch_4.pth\nEpoch [5/20], Step [100/782], Loss: 0.4390\nEpoch [5/20], Step [200/782], Loss: 0.5495\nEpoch [5/20], Step [300/782], Loss: 0.8308\nEpoch [5/20], Step [400/782], Loss: 0.6278\nEpoch [5/20], Step [500/782], Loss: 0.5586\nEpoch [5/20], Step [600/782], Loss: 0.5826\nEpoch [5/20], Step [700/782], Loss: 0.5781\nCheckpoint saved: checkpoint_epoch_5.pth\nEpoch [6/20], Step [100/782], Loss: 0.7276\nEpoch [6/20], Step [200/782], Loss: 0.5406\nEpoch [6/20], Step [300/782], Loss: 0.8206\nEpoch [6/20], Step [400/782], Loss: 0.5823\nEpoch [6/20], Step [500/782], Loss: 0.6895\nEpoch [6/20], Step [600/782], Loss: 0.5588\nEpoch [6/20], Step [700/782], Loss: 0.5841\nCheckpoint saved: checkpoint_epoch_6.pth\nEpoch [7/20], Step [100/782], Loss: 0.4680\nEpoch [7/20], Step [200/782], Loss: 0.5424\nEpoch [7/20], Step [300/782], Loss: 0.4057\nEpoch [7/20], Step [400/782], Loss: 0.4986\nEpoch [7/20], Step [500/782], Loss: 0.3263\nEpoch [7/20], Step [600/782], Loss: 0.2421\nEpoch [7/20], Step [700/782], Loss: 0.7292\nCheckpoint saved: checkpoint_epoch_7.pth\nEpoch [8/20], Step [100/782], Loss: 0.4877\nEpoch [8/20], Step [200/782], Loss: 0.5044\nEpoch [8/20], Step [300/782], Loss: 0.5692\nEpoch [8/20], Step [400/782], Loss: 0.5450\nEpoch [8/20], Step [500/782], Loss: 0.4644\nEpoch [8/20], Step [600/782], Loss: 0.5292\nEpoch [8/20], Step [700/782], Loss: 0.5596\nCheckpoint saved: checkpoint_epoch_8.pth\nEpoch [9/20], Step [100/782], Loss: 0.5010\nEpoch [9/20], Step [200/782], Loss: 0.3579\nEpoch [9/20], Step [300/782], Loss: 0.4216\nEpoch [9/20], Step [400/782], Loss: 0.4857\nEpoch [9/20], Step [500/782], Loss: 0.4492\nEpoch [9/20], Step [600/782], Loss: 0.5589\nEpoch [9/20], Step [700/782], Loss: 0.3727\nCheckpoint saved: checkpoint_epoch_9.pth\nEpoch [10/20], Step [100/782], Loss: 0.4957\nEpoch [10/20], Step [200/782], Loss: 0.3808\nEpoch [10/20], Step [300/782], Loss: 0.2800\nEpoch [10/20], Step [400/782], Loss: 0.5839\nEpoch [10/20], Step [500/782], Loss: 0.6450\nEpoch [10/20], Step [600/782], Loss: 0.2485\nEpoch [10/20], Step [700/782], Loss: 0.3191\nCheckpoint saved: checkpoint_epoch_10.pth\nEpoch [11/20], Step [100/782], Loss: 0.4981\nEpoch [11/20], Step [200/782], Loss: 0.1969\nEpoch [11/20], Step [300/782], Loss: 0.4803\nEpoch [11/20], Step [400/782], Loss: 0.4968\nEpoch [11/20], Step [500/782], Loss: 0.2868\nEpoch [11/20], Step [600/782], Loss: 0.3060\nEpoch [11/20], Step [700/782], Loss: 0.3223\nCheckpoint saved: checkpoint_epoch_11.pth\nEpoch [12/20], Step [100/782], Loss: 0.5563\nEpoch [12/20], Step [200/782], Loss: 0.3240\nEpoch [12/20], Step [300/782], Loss: 0.4879\nEpoch [12/20], Step [400/782], Loss: 0.3432\nEpoch [12/20], Step [500/782], Loss: 0.3596\nEpoch [12/20], Step [600/782], Loss: 0.3331\nEpoch [12/20], Step [700/782], Loss: 0.3999\nCheckpoint saved: checkpoint_epoch_12.pth\nEpoch [13/20], Step [100/782], Loss: 0.2076\nEpoch [13/20], Step [200/782], Loss: 0.3597\nEpoch [13/20], Step [300/782], Loss: 0.3553\nEpoch [13/20], Step [400/782], Loss: 0.2783\nEpoch [13/20], Step [500/782], Loss: 0.5880\nEpoch [13/20], Step [600/782], Loss: 0.1713\nEpoch [13/20], Step [700/782], Loss: 0.2293\nCheckpoint saved: checkpoint_epoch_13.pth\nEpoch [14/20], Step [100/782], Loss: 0.3046\nEpoch [14/20], Step [200/782], Loss: 0.4573\nEpoch [14/20], Step [300/782], Loss: 0.3003\nEpoch [14/20], Step [400/782], Loss: 0.2278\nEpoch [14/20], Step [500/782], Loss: 0.3094\nEpoch [14/20], Step [600/782], Loss: 0.4682\nEpoch [14/20], Step [700/782], Loss: 0.4315\nCheckpoint saved: checkpoint_epoch_14.pth\nEpoch [15/20], Step [100/782], Loss: 0.2430\nEpoch [15/20], Step [200/782], Loss: 0.2210\nEpoch [15/20], Step [300/782], Loss: 0.3097\nEpoch [15/20], Step [400/782], Loss: 0.2182\nEpoch [15/20], Step [500/782], Loss: 0.4033\nEpoch [15/20], Step [600/782], Loss: 0.1977\nEpoch [15/20], Step [700/782], Loss: 0.3574\nCheckpoint saved: checkpoint_epoch_15.pth\nEpoch [16/20], Step [100/782], Loss: 0.3435\nEpoch [16/20], Step [200/782], Loss: 0.3160\nEpoch [16/20], Step [300/782], Loss: 0.2520\nEpoch [16/20], Step [400/782], Loss: 0.2335\nEpoch [16/20], Step [500/782], Loss: 0.2684\nEpoch [16/20], Step [600/782], Loss: 0.1287\nEpoch [16/20], Step [700/782], Loss: 0.2633\nCheckpoint saved: checkpoint_epoch_16.pth\nEpoch [17/20], Step [100/782], Loss: 0.2660\nEpoch [17/20], Step [200/782], Loss: 0.4231\nEpoch [17/20], Step [300/782], Loss: 0.2082\nEpoch [17/20], Step [400/782], Loss: 0.1971\nEpoch [17/20], Step [500/782], Loss: 0.3859\nEpoch [17/20], Step [600/782], Loss: 0.2501\nEpoch [17/20], Step [700/782], Loss: 0.2989\nCheckpoint saved: checkpoint_epoch_17.pth\nEpoch [18/20], Step [100/782], Loss: 0.1683\nEpoch [18/20], Step [200/782], Loss: 0.1966\nEpoch [18/20], Step [300/782], Loss: 0.3517\nEpoch [18/20], Step [400/782], Loss: 0.2039\nEpoch [18/20], Step [500/782], Loss: 0.1471\nEpoch [18/20], Step [600/782], Loss: 0.1772\nEpoch [18/20], Step [700/782], Loss: 0.3516\nCheckpoint saved: checkpoint_epoch_18.pth\nEpoch [19/20], Step [100/782], Loss: 0.3305\nEpoch [19/20], Step [200/782], Loss: 0.2628\nEpoch [19/20], Step [300/782], Loss: 0.3142\nEpoch [19/20], Step [400/782], Loss: 0.0818\nEpoch [19/20], Step [500/782], Loss: 0.2725\nEpoch [19/20], Step [600/782], Loss: 0.1282\nEpoch [19/20], Step [700/782], Loss: 0.2287\nCheckpoint saved: checkpoint_epoch_19.pth\nEpoch [20/20], Step [100/782], Loss: 0.1750\nEpoch [20/20], Step [200/782], Loss: 0.2436\nEpoch [20/20], Step [300/782], Loss: 0.2883\nEpoch [20/20], Step [400/782], Loss: 0.1443\nEpoch [20/20], Step [500/782], Loss: 0.2454\nEpoch [20/20], Step [600/782], Loss: 0.1285\nEpoch [20/20], Step [700/782], Loss: 0.2181\nCheckpoint saved: checkpoint_epoch_20.pth\n","output_type":"stream"}]},{"cell_type":"code","source":"# 测试模型\nmodel.eval()\nwith torch.no_grad():\n    correct = 0\n    total = 0\n    for images, labels in test_loader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    print(f'Accuracy of the model on the 10000 test images: {100 * correct / total:.2f}%')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-30T15:51:09.476430Z","iopub.execute_input":"2024-09-30T15:51:09.477188Z","iopub.status.idle":"2024-09-30T15:51:31.960961Z","shell.execute_reply.started":"2024-09-30T15:51:09.477145Z","shell.execute_reply":"2024-09-30T15:51:31.959969Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Accuracy of the model on the 10000 test images: 87.21%\n","output_type":"stream"}]},{"cell_type":"code","source":"# # 加载已保存的模型检查点\n# def load_model(model, optimizer):\n#     if os.path.isfile(checkpoint_path):\n#         checkpoint = torch.load(checkpoint_path)\n#         model.load_state_dict(checkpoint['model_state_dict'])\n#         optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n#         print(\"Model and optimizer state loaded from checkpoint.\")\n#     else:\n#         print(\"No checkpoint found.\")\n\n# # 图像分类函数\n# def classify_image(img_path, model):\n#     model.eval()  # 设置为评估模式\n#     with torch.no_grad():\n#         img = Image.open(img_path)\n#         img = transform(img).unsqueeze(0).to(device)  # 添加批次维度并移动到设备\n#         outputs = model(img)\n#         _, predicted = torch.max(outputs.data, 1)\n#         return predicted.item()\n\n# # 示例：加载模型并分类单张图片\n# model = ResNet(BasicBlock)\n# optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n# # 加载模型\n# load_model(model, optimizer)\n\n# # 替换为你的图片路径\n# image_path = 'path_to_your_image.jpg'\n# predicted_class = classify_image(image_path, model)\n# print(f'Predicted class for the image: {predicted_class}')\nprint('ok')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-30T15:54:07.682820Z","iopub.execute_input":"2024-09-30T15:54:07.683504Z","iopub.status.idle":"2024-09-30T15:54:07.688276Z","shell.execute_reply.started":"2024-09-30T15:54:07.683461Z","shell.execute_reply":"2024-09-30T15:54:07.687383Z"},"trusted":true},"execution_count":12,"outputs":[]}]}